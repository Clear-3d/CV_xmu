{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c69a978d",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "## Introduction\n",
    "In this Assignment, we will implement a transformer model to finish a translation task between English and Chinese step by step. Before we start, you are suggested to read the original paper, the lecture of Hung-yi Lee and the blogs of the transformers. It may take much time to do those, however, only in this way, can you get the deep understanding of the task.\n",
    "\n",
    "### the original paper\n",
    "- paper: https://arxiv.org/pdf/1706.03762.pdf\n",
    "\n",
    "### transformer blog: \n",
    "- https://ketanhdoshi.github.io/Transformers-Overview/\n",
    "- https://ketanhdoshi.github.io/Transformers-Arch/\n",
    "- https://ketanhdoshi.github.io/Transformers-Attention/\n",
    "- https://ketanhdoshi.github.io/Transformers-Why/\n",
    "\n",
    "\n",
    "### the lecture of Hung-yi Lee\n",
    "\n",
    "- bilibili: https://www.bilibili.com/video/BV1v3411r78R/?spm_id_from=333.337.search-card.all.click&vd_source=155ff7fe8c811c0bd4176244f231e86b\n",
    " \n",
    "- slides: https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/self_v7.pdf\n",
    "\n",
    "Also, here are some implementations of the other frameworks which you can refer.\n",
    "\n",
    "- [implement of keras](https://keras.io/examples/nlp/neural_machine_translation_with_transformer/)\n",
    "- [implement of huggingface](https://huggingface.co/learn/nlp-course/chapter7/4?fw=pt)\n",
    "## Download dataset\n",
    "Let's start with the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4c51a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check your development environment.\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "def beijing(sec, what):\n",
    "    beijing_time = datetime.datetime.now() + datetime.timedelta(hours=8)\n",
    "    return beijing_time.timetuple()\n",
    "\n",
    "\n",
    "logging.Formatter.converter = beijing\n",
    "# set log\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s: %(message)s',\n",
    "                     datefmt='%Y-%m-%d %H:%M:%S',)\n",
    "\n",
    "logging.info('The version information:')\n",
    "logging.info(f'Python: {sys.version}')\n",
    "logging.info(f'PyTorch: {torch.__version__}')\n",
    "assert torch.cuda.is_available() == True, 'Please finish your GPU develop environment'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baaa8aa",
   "metadata": {},
   "source": [
    "## Fix random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7297cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "seed = 2023\n",
    "\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  \n",
    "np.random.seed(seed)  \n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "logging.info(f'The random seed is fixed to {seed}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacf92da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download and unzip files\n",
    "import requests\n",
    "\n",
    "#define download function\n",
    "def download(url, save_dir = Path.cwd()):\n",
    "    os.makedirs(save_dir, exist_ok = True)\n",
    "    \n",
    "    file_name = url.split('/')[-1]\n",
    "    file_path = save_dir / file_name\n",
    "    if file_path.exists():\n",
    "        logging.info(f'{file_name} exists!')\n",
    "        return \n",
    "    logging.info(f'downloading {file_name} from {url}')\n",
    "    response = requests.get(url)\n",
    "    if response.ok:\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        logging.info(f\"download  {file_name} from {url} successfully!\")\n",
    "    else:\n",
    "        print(f\"Fail to download  {file_name} from {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63934242",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = 'http://data.statmt.org/wmt18/translation-task/training-parallel-nc-v13.tgz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf35ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = Path('dataset')\n",
    "download(dataset_url, save_dir=dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407dcb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "tgz_file_path = dataset_dir / 'training-parallel-nc-v13.tgz'\n",
    "dataset_path = dataset_dir / 'training-parallel-nc-v13'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d31fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = tarfile.open(tgz_file_path)\n",
    "if not dataset_path.exists():\n",
    "    logging.info(f\"exact {tgz_file_path} to {dataset_dir}\")\n",
    "    tar.extractall(dataset_dir)\n",
    "else:\n",
    "    logging.info(f\"{dataset_path} exists!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4c5fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_data_path = dataset_path / 'news-commentary-v13.zh-en.zh'\n",
    "english_data_path = dataset_path / 'news-commentary-v13.zh-en.en'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78dad02",
   "metadata": {},
   "source": [
    "## Get corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7c923c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_lines = []\n",
    "english_lines = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76467b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_data_file = open(chinese_data_path, 'r')\n",
    "english_data_file = open(english_data_path, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2948f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_data_list = list(chinese_data_file.readlines())\n",
    "english_data_list = list(english_data_file.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf94c6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert  len(chinese_data_list) == len(english_data_list) and len(chinese_data_list) == 252777, \\\n",
    "    'The number of sample error! Please load the dataset again'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efa1c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_samples = 5\n",
    "index = 0\n",
    "for chinese_sentence, english_sentence in zip(chinese_data_list, english_data_list):\n",
    "    print(index, '\\n Chinese sentence: ' + chinese_sentence, 'English sentence: ' , english_sentence)\n",
    "    index = index + 1\n",
    "    if index > number_of_samples:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1472249",
   "metadata": {},
   "source": [
    "## Dataset division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e84c348",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_list = []\n",
    "for chinese_sentence, english_sentence in zip(chinese_data_list, english_data_list):\n",
    "    dataset_list.append([english_sentence.replace('\\n',''), chinese_sentence.replace('\\n','')])\n",
    "print(dataset_list[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13281155",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# train:test:dev = 8:1:1\n",
    "train_dataset, test_and_dev_dataset = train_test_split(dataset_list, shuffle=True, test_size=0.2, random_state=2023)\n",
    "test_dataset, dev_dataset = train_test_split(test_and_dev_dataset, shuffle=True, test_size=0.5, random_state=2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bbf8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3a686a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461d1151",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f593c3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset), len(test_dataset), len(dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0c1f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_dir_path = Path('dataset')\n",
    "Path.mkdir(json_dir_path, exist_ok=True)\n",
    "\n",
    "dataset_list = [train_dataset, test_dataset, dev_dataset]\n",
    "json_name_list = ['train.json', 'test.json', 'dev.json']\n",
    "for dataset, json_name in zip(dataset_list, json_name_list):\n",
    "    dataset_path = json_dir_path / json_name\n",
    "    json_data = json.dumps(dataset)\n",
    "    with open(dataset_path, \"w\",encoding = 'utf-8') as file:\n",
    "        file.write(json_data)\n",
    "    print(f'save {json_name} to {dataset_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54afc698",
   "metadata": {},
   "source": [
    "## Tokenize\n",
    "The step of Tokenizing is a very important step in the natrual language process(NLP) field. Please refer to [blog:why and how to tokenize](https://huggingface.co/learn/nlp-course/chapter6/1?fw=pt) to learn more about this process. \n",
    "[SentencePiece](https://github.com/google/sentencepiece/) is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. SentencePiece implements subword units (e.g., byte-pair-encoding (BPE) [Sennrich et al.]) and unigram language model [Kudo.]) with the extension of direct training from raw sentences. SentencePiece allows us to make a purely end-to-end system that does not depend on language-specific pre/postprocessing.\n",
    "In this assignment, we will use the Byte-Pair Encoding([BPE](https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt)) tokenization. Byte-Pair Encoding (BPE) was initially developed as an algorithm to compress texts, and then used by OpenAI for tokenization when pretraining the GPT model. It’s used by a lot of Transformer models, including GPT, GPT-2, RoBERTa, BART, and DeBERTa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d4cfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "def train(input_file, vocab_size, model_name, model_type, character_coverage):\n",
    "    \"\"\"\n",
    "    search on https://github.com/google/sentencepiece/blob/master/doc/options.md to learn more about the parameters\n",
    "    :param input_file: one-sentence-per-line raw corpus file. No need to run tokenizer, normalizer or preprocessor.\n",
    "                       By default, SentencePiece normalizes the input with Unicode NFKC.\n",
    "                       You can pass a comma-separated list of files.\n",
    "    :param vocab_size: vocabulary size, e.g., 8000, 16000, or 32000\n",
    "    :param model_name: output model name prefix. <model_name>.model and <model_name>.vocab are generated.\n",
    "    :param model_type: model type. Choose from unigram (default), bpe, char, or word.\n",
    "                       The input sentence must be pretokenized when using word type.\n",
    "    :param character_coverage: amount of characters covered by the model, good defaults are: 0.9995 for languages with\n",
    "                               rich character set like Japanse or Chinese and 1.0 for other languages with\n",
    "                               small character set.\n",
    "    \"\"\"\n",
    "    input_argument = '--input=%s --model_prefix=%s --vocab_size=%s --model_type=%s --character_coverage=%s ' \\\n",
    "                     '--pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3 '\n",
    "    cmd = input_argument % (input_file, model_name, vocab_size, model_type, character_coverage)\n",
    "    cmd = 'spm_train '+ cmd\n",
    "    # spm.SentencePieceTrainer.Train(cmd)\n",
    "    # in this assignment, we use the following command\n",
    "    os.system(cmd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5379b737",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_dir  = Path('tokenizer')\n",
    "os.makedirs(tokenizer_dir, exist_ok = True)\n",
    "eng_model_path = tokenizer_dir / Path('eng.model')\n",
    "eng_vocab_path = tokenizer_dir /Path('eng.vocab')\n",
    "chn_model_path = tokenizer_dir /Path('chn.model')\n",
    "chn_vocab_path = tokenizer_dir / Path('chn.vocab')\n",
    "eng_model_path.exists(),eng_vocab_path.exists(),chn_model_path.exists(), chn_vocab_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200046d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_input = english_data_path\n",
    "en_vocab_size = 32000\n",
    "en_model_name = tokenizer_dir / Path('eng')\n",
    "en_model_type = 'bpe'\n",
    "en_character_coverage = 1\n",
    "\n",
    "tokenizer_dir  = Path('tokenizer')\n",
    "eng_model_path = tokenizer_dir / Path('eng.model')\n",
    "eng_vocab_path = tokenizer_dir /Path('eng.vocab')\n",
    "\n",
    "if eng_model_path.exists() and eng_vocab_path.exists():\n",
    "    logging.info(f\"{eng_model_path } and {eng_vocab_path} have exist! continue run the code\")\n",
    "else:\n",
    "    train(en_input, en_vocab_size, en_model_name, en_model_type, en_character_coverage)\n",
    "\n",
    "ch_input = chinese_data_path\n",
    "ch_vocab_size = 32000\n",
    "ch_model_name = tokenizer_dir / Path('chn')\n",
    "ch_model_type = 'bpe'\n",
    "ch_character_coverage = 0.9995\n",
    "\n",
    "chn_model_path = tokenizer_dir / Path('chn.model')\n",
    "chn_vocab_path = tokenizer_dir / Path('chn.vocab')\n",
    "if chn_model_path.exists() and chn_vocab_path.exists():\n",
    "    logging.info(f\"{chn_model_path } and {chn_vocab_path} have exist! continue run the code\")\n",
    "else:\n",
    "    train(ch_input, ch_vocab_size, ch_model_name, ch_model_type, ch_character_coverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9429cd74",
   "metadata": {},
   "source": [
    "## SentencePiece test\n",
    "After we finish the sentencepiece training, let's do some test to get the understanding of its work processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05053db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "text = \"美国总统特朗普今日抵达夏威夷。\"\n",
    "\n",
    "sp.Load('./tokenizer/chn.model')\n",
    "print(sp.EncodeAsPieces(text))\n",
    "\n",
    "# encode the text\n",
    "s =sp.EncodeAsIds(text)\n",
    "# embeding vector\n",
    "print(s)\n",
    "# decode the embedding vector\n",
    "print(sp.decode_ids(s))\n",
    "\n",
    "# let's do little change to the embeding functio vector\n",
    "for i in range(0,len(s),2):\n",
    "    print(f'{i}: {s[i]} --> {s[i] + 1}')\n",
    "    s[i] = s[i] + 1 \n",
    "# look new vector\n",
    "print(s)\n",
    "# decode the new embedding vector\n",
    "print(sp.decode_ids(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4180ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "text = \"U.S. President Donald Trump arrived in Hawaii today.\"\n",
    "\n",
    "# do same as above, but English\n",
    "sp.Load('./tokenizer/eng.model')\n",
    "print(sp.EncodeAsPieces(text))\n",
    "s =sp.EncodeAsIds(text)\n",
    "print(s)\n",
    "print(sp.decode_ids(s))\n",
    "\n",
    "for i in range(0,len(s),2):\n",
    "    print(f'{i}: {s[i]} --> {s[i] + 1}')\n",
    "    s[i] = s[i] + 1 \n",
    "print(s)\n",
    "print(sp.decode_ids(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499ca7be",
   "metadata": {},
   "source": [
    "## Config\n",
    "Here is the configuration of this assignment, you are only allowed to change these parameters:\n",
    "- batch_size\n",
    "- epoch_num\n",
    "- lr\n",
    "- beam size\n",
    "- gpu_id\n",
    "- device_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0438dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import torch\n",
    "\n",
    "dataset_path = Path('dataset')\n",
    "experiment_path = Path('experiment')\n",
    "Path.mkdir(dataset_path, exist_ok=True)\n",
    "Path.mkdir(experiment_path, exist_ok=True)\n",
    "\n",
    "config = Namespace(\n",
    "d_model = 512,\n",
    "n_heads = 8,\n",
    "n_layers = 6,\n",
    "d_k = 64,\n",
    "d_v = 64,\n",
    "d_ff = 2048,\n",
    "dropout = 0.1,\n",
    "padding_idx = 0,\n",
    "bos_idx = 2,\n",
    "eos_idx = 3,\n",
    "src_vocab_size = 32000,\n",
    "tgt_vocab_size = 32000,\n",
    "batch_size = 256,\n",
    "epoch_num = 100,\n",
    "early_stop = 5,\n",
    "lr = 3e-4,\n",
    "\n",
    "# the max length of sentence in greed decode\n",
    "max_len = 60,\n",
    "# beam size for bleu\n",
    "beam_size = 3,\n",
    "# Label Smoothing\n",
    "use_smoothing = False,\n",
    "# NoamOpt\n",
    "use_noamopt = True,\n",
    "\n",
    "train_data_path = dataset_path / 'train.json',\n",
    "dev_data_path = dataset_path / 'dev.json',\n",
    "test_data_path = dataset_path / 'test.json',\n",
    "output_model_path = experiment_path / 'model.pth',\n",
    "log_path = experiment_path / 'train.log',\n",
    "output_path = experiment_path / 'output.txt',\n",
    "\n",
    "# gpu_id and device id is the relative id\n",
    "# thus, if you wanna use os.environ['CUDA_VISIBLE_DEVICES'] = '2, 3'\n",
    "# you should set CUDA_VISIBLE_DEVICES = 2 as main -> gpu_id = '0', device_id = [0, 1]\n",
    "gpu_id = '0',\n",
    "device_id = [0, 1, 2, 3, 4, 5],\n",
    ")\n",
    "# set device\n",
    "if config.gpu_id != '':\n",
    "    device = torch.device(f\"cuda:{config.gpu_id}\")\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaf34e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5b2911",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8993321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "def chinese_tokenizer_load():\n",
    "    sp_chn = spm.SentencePieceProcessor()\n",
    "    sp_chn.Load('./tokenizer/chn.model')\n",
    "    return sp_chn\n",
    "\n",
    "\n",
    "def english_tokenizer_load():\n",
    "    sp_eng = spm.SentencePieceProcessor()\n",
    "    sp_eng.Load('./tokenizer/eng.model')\n",
    "    return sp_eng\n",
    "\n",
    "def set_logger(log_path):\n",
    "    \"\"\"Set the logger to log info in terminal and file `log_path`.\n",
    "    In general, it is useful to have a logger so that every output to the terminal is saved\n",
    "    in a permanent file. Here we save it to `model_dir/train.log`.\n",
    "    Example:\n",
    "    ```\n",
    "    logging.info(\"Starting training...\")\n",
    "    ```\n",
    "    Args:\n",
    "        log_path: (string) where to log\n",
    "    \"\"\"\n",
    "    if os.path.exists(log_path) is True:\n",
    "        os.remove(log_path)\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    if not logger.handlers:\n",
    "        # Logging to a file\n",
    "        file_handler = logging.FileHandler(log_path)\n",
    "        file_handler.setFormatter(logging.Formatter('%(asctime)s:%(levelname)s: %(message)s'))\n",
    "        logger.addHandler(file_handler)\n",
    "\n",
    "        # Logging to console\n",
    "        stream_handler = logging.StreamHandler()\n",
    "        stream_handler.setFormatter(logging.Formatter('%(message)s'))\n",
    "        logger.addHandler(stream_handler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fbcbfa",
   "metadata": {},
   "source": [
    "## Finish the Class: MTDataset(10 marks)\n",
    "You are supposed to finish the code of the class **MTDataset** in following block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7881407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "DEVICE = device\n",
    "\n",
    "\n",
    "def subsequent_mask(size): \n",
    "    \"\"\"Mask out subsequent positions.\"\"\"\n",
    "    # set the shape of subsequent_mask matrix\n",
    "    attn_shape = (1, size, size)\n",
    "\n",
    "    # create a subsequent_mask matrix with ones in the upper right corner (excluding the main diagonal) and zeros in the lower left corner (including the main diagonal).\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "\n",
    "    # return a subsequent_mask matrix with False in the upper right corner (excluding the main diagonal) and True in the lower left corner (including the main diagonal).\n",
    "    return torch.from_numpy(subsequent_mask) == 0\n",
    "\n",
    "\n",
    "class Batch:\n",
    "    \"\"\"Object for holding a batch of data with mask during training.\"\"\"\n",
    "    def __init__(self, src_text, trg_text, src, trg=None, pad=0):\n",
    "        self.src_text = src_text\n",
    "        self.trg_text = trg_text\n",
    "        src = src.to(DEVICE)\n",
    "        self.src = src\n",
    "        # Determine the non-empty part of the current input sentence as a bool sequence.\n",
    "        # And add one dimension in front of seq length to form a matrix of dimension 1×seq length\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        # If the output target is not null, then you need to mask the target clause to be used by the decoder.\n",
    "        if trg is not None:\n",
    "            trg = trg.to(DEVICE)\n",
    "            # Target input part to be used by decoder\n",
    "            self.trg = trg[:, :-1]\n",
    "            # The decoder training should predict the output target result\n",
    "            self.trg_y = trg[:, 1:]\n",
    "            # Attention mask the target input portion\n",
    "            self.trg_mask = self.make_std_mask(self.trg, pad)\n",
    "            # Counts the actual number of words in the target results that should be outputted\n",
    "            self.ntokens = (self.trg_y != pad).data.sum()\n",
    "\n",
    "    # Mask\n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"\"\"Create a mask to hide padding and future words.\"\"\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & Variable(subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "        return tgt_mask\n",
    "\n",
    "\n",
    "class MTDataset(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        self.out_en_sent, self.out_cn_sent = self.get_dataset(data_path, sort=True)\n",
    "        self.sp_eng = english_tokenizer_load()\n",
    "        self.sp_chn = chinese_tokenizer_load()\n",
    "        self.PAD = self.sp_eng.pad_id()  # 0\n",
    "        self.BOS = self.sp_eng.bos_id()  # 2\n",
    "        self.EOS = self.sp_eng.eos_id()  # 3\n",
    "\n",
    "    @staticmethod\n",
    "    def len_argsort(seq):\n",
    "        \"\"\"\n",
    "        Input: A list of tokenized sentences.\n",
    "        Output: Indices that would sort the sentences by their lengths.\n",
    "\n",
    "        This static method takes in a list of tokenized sentences and returns \n",
    "        a list of indices that would sort the sentences by their lengths.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def get_dataset(self, data_path, sort=False):\n",
    "        \"\"\"Sort Chinese and English in the same order, using the English sentence length ordering (sentence subscripts) as the base.\"\"\"\n",
    "        logging.info(f\"get_dataset from:{os.path.abspath(data_path)}\")\n",
    "        dataset = json.load(open(data_path, 'r'))\n",
    "        out_en_sent = []\n",
    "        out_cn_sent = []\n",
    "        for idx, _ in enumerate(dataset):\n",
    "            out_en_sent.append(dataset[idx][0])\n",
    "            out_cn_sent.append(dataset[idx][1])\n",
    "        if sort:\n",
    "            sorted_index = self.len_argsort(out_en_sent)\n",
    "            out_en_sent = [out_en_sent[i] for i in sorted_index]\n",
    "            out_cn_sent = [out_cn_sent[i] for i in sorted_index]\n",
    "        return out_en_sent, out_cn_sent\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        eng_text = self.out_en_sent[idx]\n",
    "        chn_text = self.out_cn_sent[idx]\n",
    "        return [eng_text, chn_text]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.out_en_sent)\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Input: A batch of data.\n",
    "        Output: A Batch object containing source and target texts, and their tokenized and padded versions.\n",
    "\n",
    "        This method is responsible for:\n",
    "        1. Extracting English and Chinese texts from the batch.\n",
    "        2. Tokenizing and padding these texts.\n",
    "        3. Returning a Batch object that holds these details.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "        return Batch(src_text, tgt_text, batch_input, batch_target, self.PAD)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e716f393",
   "metadata": {},
   "source": [
    "## Finish the implement of Label Smoothing, Embeddings and Softmax, Positional Encoding, Attention and Position-wise Feed-Forward Networks(20 marks)\n",
    "### Label Smoothing\n",
    "Label Smoothing hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n",
    "We implement label smoothing using the KL div loss. Instead of using a one-hot target distribution, we create a distribution that has confidence of the correct word and the rest of the smoothing mass distributed throughout the vocabulary\n",
    "\n",
    "### Embeddings and Softmax\n",
    "Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension$d_model$.\n",
    "We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to (cite)[https://arxiv.org/abs/1608.05859]. In the embedding layers, we multiply those weights by $\\sqrt d_{model}$\n",
    "\n",
    "### Positional Encoding\n",
    "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add “positional encodings” to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension$\\sqrt d_{model}$. as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed (cite)[https://arxiv.org/pdf/1705.03122.pdf]\n",
    "In this work, we use sine and cosine functions of different frequencies\n",
    "$$\n",
    "\\begin{equation*}\n",
    "PE_{pos, 2i} = sin(pos/10000^{2i/d_{model}}) \n",
    "\\end{equation*}\n",
    "$$\n",
    "$$\n",
    "\\begin{equation*}\n",
    "PE_{pos, 2i+1} = cos(pos/10000^{2i/d_{model}}) \n",
    "\\end{equation*}\n",
    "$$\n",
    "where $pos$ is the position and $i$ is the dimension.That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from $2\\pi$ to $1000 * 2\\pi$. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset $k$, $PE_{pos + k}$ can be represented as a linear function of $PE_{pos}$.\n",
    "\n",
    "\n",
    "In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of $P_{drop} = 0.1$\n",
    "\n",
    "### Attention\n",
    "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
    "\n",
    "### Position-wise Feed-Forward Networks\n",
    "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2648313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LabelSmoothing(nn.Module):\n",
    "    \"\"\"Implement label smoothing.\"\"\"\n",
    "\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(size_average=False)\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad=False))\n",
    "\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        # Embedding layer\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        # Embedding dimension \n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input: Tensor 'x' containing token indices.\n",
    "        Output: The corresponding embedding matrix, scaled by the square root of the embedding dimension.\n",
    "        \n",
    "        Fetches the embeddings for the given token indices and scales the result by \n",
    "        the square root of the embedding dimension.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Initialize an all-zero matrix with a size of max_len (the set maximum length) × embedding dimension.\n",
    "        # To hold the positional embedding of all positions less than this length.\n",
    "        pe = torch.zeros(max_len, d_model, device=DEVICE)\n",
    "        # Generate a position subscripted tensor matrix (each row is a position subscript)\n",
    "        \"\"\"\n",
    "        Forms like:\n",
    "        tensor([[0.],\n",
    "                [1.],\n",
    "                [2.],\n",
    "                [3.],\n",
    "                [4.],\n",
    "                ...])\n",
    "        \"\"\"\n",
    "        position = torch.arange(0., max_len, device=DEVICE).unsqueeze(1)\n",
    "        # Here the power operation is too much, we use exp and log to convert the denominator to be divided below the pos in the realization formula \n",
    "        # Be careful with the negative sign since it is the denominator\n",
    "        div_term = torch.exp(torch.arange(0., d_model, 2, device=DEVICE) * -(math.log(10000.0) / d_model))\n",
    "\n",
    "        # According to the formula, the positional texture values of each position in each embedding dimension are calculated and stored into the pe matrix\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Add 1 dimension so that the pe dimension becomes: 1 x max_len x embedding dimension\n",
    "        # (to facilitate subsequent batch summing of embedding of all words of a sentence with a batch)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        # Save the pe matrix in a persistent buffer state (will not be used as a parameter to be trained)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input: Tensor 'x' containing embeddings.\n",
    "        Output: Tensor with positional encodings added to the embeddings.\n",
    "        \n",
    "        Process:\n",
    "        1. Add positional encodings to the given embeddings 'x'.\n",
    "        2. Ensure the positional encodings are aligned with the sequence length of 'x'.\n",
    "        3. Apply dropout (defined in __init__) before returning.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"\"\"\n",
    "    Compute the scaled dot product attention.\n",
    "    \n",
    "    Input:\n",
    "    - query, key, value: Tensors for the query, key, and value\n",
    "    - mask: Optional tensor to mask certain values \n",
    "    - dropout: Optional dropout layer for regularization\n",
    "    \n",
    "    Steps:\n",
    "    1. Calculate 'scores' by computing the dot product of 'query' and 'key'. Don't forget to scale it.\n",
    "    2. If a mask is provided, apply it to the 'scores' tensor. The idea is to set masked positions to a large negative value.\n",
    "    3. Apply softmax to the 'scores' to get the attention probabilities.\n",
    "    4. If a dropout layer is provided, apply dropout to the attention probabilities.\n",
    "    5. Finally, compute the output by multiplying the attention probabilities with 'value'.\n",
    "    \n",
    "    Returns:\n",
    "    - The result tensor and the attention probabilities.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        # Guaranteed to be divisible\n",
    "        assert d_model % h == 0\n",
    "        # Get a HEAD's ATTENTION representation of the dimension\n",
    "        self.d_k = d_model // h\n",
    "        # Number of heads\n",
    "        self.h = h\n",
    "        # Define 4 fully connected functions for subsequent use as the WQ, WK, WV matrices and the last h polytopic attention matrices to be transformed after concat\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Forward propagation for multi-headed attention.\n",
    "        \n",
    "        Input:\n",
    "        - query, key, value: Tensors for query, key, and value.\n",
    "        - mask: Optional tensor to mask certain values.\n",
    "        \n",
    "        Steps:\n",
    "        1. If mask is provided, adjust its shape.\n",
    "        2. Find the batch size from the 'query' tensor.\n",
    "        3. Apply the WQ, WK, WV transformations to query, key, and value respectively.\n",
    "        4. Split the transformed tensors into 'h' blocks.\n",
    "        5. For each block, calculate the attention values.\n",
    "        6. Concatenate all the attention blocks.\n",
    "        7. Apply the final linear transformation.\n",
    "        \n",
    "        Returns:\n",
    "        - The output tensor after multi-headed attention.\n",
    "        \n",
    "        Note: You might want to revisit the 'attention' function you implemented before.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        # Initialize α to all 1's and β to all 0's.\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        # smooth term (in calculus)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Calculate mean and variance by last dimension\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "\n",
    "        # Returns the result of Layer Norm\n",
    "        return self.a_2 * (x - mean) / torch.sqrt(std ** 2 + self.eps) + self.b_2\n",
    "\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    The role of SublayerConnection is to connect the Multi-Head Attention and Feed Forward layers together.\n",
    "    Only after the output of each layer, you have to do the Layer Norm first and then the residual connection.\n",
    "    Sublayer is a lambda function\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        # Returns the result after joining the Layer Norm and the residuals.\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "\n",
    "def clones(module, N):\n",
    "    \"\"\"Clone model block, cloned model block parameters are not shared\"\"\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The implementation of Feed Forward in Encoder and Deocder, which mainly contains a multilayer perceptron\n",
    "        args: \n",
    "        d_model：the input dimension of Encoder\n",
    "        d_ff：the intermediate dimension\n",
    "        dropout：the rate of dropout\n",
    "\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13118ffa",
   "metadata": {},
   "source": [
    "## Finish the implement of Encoder and EncoderLayer(10 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2078bd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, encoderlayer, N):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        The implementation of Encoder, note that the core is a stack of N encoder(layers)\n",
    "        args：\n",
    "        encoderlayer：the implementation of encoder in Encoder\n",
    "        N：the number of encoder in Encoder, such as 6\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, multihead_attn, feed_forward, dropout):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        The implementation of encoder in Encoder, which is made up of self-attention layer, feed forward and norm layer etc\n",
    "        args：\n",
    "        d_model：the input dimension of Encoder\n",
    "        multihead_attn：multihead attention module in encoder\n",
    "        feed_forward：the feed forward module in encoder\n",
    "        dropout：the rate of dropout\n",
    "        x：the input of Encoder\n",
    "        mask：the mask of multihead attention\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10acc45a",
   "metadata": {},
   "source": [
    "## Finish the implement of Decoder and DecoderLayer(10 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0495b79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, decoderlayer, N):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        The implementation of Decoder, the core is a stack of N decoder(layers)\n",
    "        args：\n",
    "        decoder layer：the implementation of decoder\n",
    "        N：the number of decoder in Decoder, such as 6\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, multihead_attn, src_attn, feed_forward, dropout):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        The implementation of a decoder in the Decoder, which is made up of self-attn, src_attn and feed forward\n",
    "        args：\n",
    "        d_model：the output dimension of Encoder\n",
    "        multihead_attn：the multihead attention module(self attention) in decoder\n",
    "        src_attn：the cross attention module in decoder\n",
    "        feed_forward：the feed forward module\n",
    "        memory：the output of Encoder\n",
    "        x：the input of Decoder\n",
    "        src_mask：the mask of cross attention module\n",
    "        tgt_mask：the mask of multihead attention module(self attention)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b08dc12",
   "metadata": {},
   "source": [
    "## Finish the implement of the whole model(10 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e41294",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        \"\"\"\n",
    "        The implementation of Transformer.\n",
    "        args：\n",
    "        encoder: the encoder of the transformer\n",
    "        decoder: the decoder of the transformer\n",
    "        src_embed: the embedding of the source sentence\n",
    "        tgt_embed: the embedding of the target sentence\n",
    "        generator: the output of the final layer of the decoder\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        \"\"\"\n",
    "        args: \n",
    "        src: the source sentence\n",
    "        src_mask: the masked source sentence \n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        \"\"\"\n",
    "        args: \n",
    "        memory: the output of encoder\n",
    "        src_mask: the masked source sentence\n",
    "        tgt: the target sentence\n",
    "        tgt_mask: the masked target sentence \n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        args:\n",
    "        src: the source sentence\n",
    "        tgt: the target sentence\n",
    "        src_mask: the masked source sentence\n",
    "        tgt_mask: the masked target sentence \n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6558c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    # vocab: tgt_vocab\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # perform the log_softmax operation (taking the logarithm of the softmax result).\n",
    "        return F.log_softmax(self.proj(x), dim=-1)\n",
    "\n",
    "\n",
    "def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    c = copy.deepcopy\n",
    "\n",
    "    attn = MultiHeadedAttention(h, d_model).to(DEVICE)\n",
    "\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout).to(DEVICE)\n",
    "\n",
    "    position = PositionalEncoding(d_model, dropout).to(DEVICE)\n",
    "\n",
    "    model = Transformer(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout).to(DEVICE), N).to(DEVICE),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout).to(DEVICE), N).to(DEVICE),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab).to(DEVICE), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab).to(DEVICE), c(position)),\n",
    "        Generator(d_model, tgt_vocab)).to(DEVICE)\n",
    "\n",
    "    # This was important from their code.\n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "\n",
    "def batch_greedy_decode(model, src, src_mask, max_len=64, start_symbol=2, end_symbol=3):\n",
    "    batch_size, src_seq_len = src.size()\n",
    "    results = [[] for _ in range(batch_size)]\n",
    "    stop_flag = [False for _ in range(batch_size)]\n",
    "    count = 0\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    tgt = torch.Tensor(batch_size, 1).fill_(start_symbol).type_as(src.data)\n",
    "\n",
    "    for s in range(max_len):\n",
    "        tgt_mask = subsequent_mask(tgt.size(1)).expand(batch_size, -1, -1).type_as(src.data)\n",
    "        out = model.decode(memory, src_mask, Variable(tgt), Variable(tgt_mask))\n",
    "\n",
    "        prob = model.generator(out[:, -1, :])\n",
    "        pred = torch.argmax(prob, dim=-1)\n",
    "\n",
    "        tgt = torch.cat((tgt, pred.unsqueeze(1)), dim=1)\n",
    "        pred = pred.cpu().numpy()\n",
    "        for i in range(batch_size):\n",
    "            # print(stop_flag[i])\n",
    "            if stop_flag[i] is False:\n",
    "                if pred[i] == end_symbol:\n",
    "                    count += 1\n",
    "                    stop_flag[i] = True\n",
    "                else:\n",
    "                    results[i].append(pred[i].item())\n",
    "            if count == batch_size:\n",
    "                break\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def greedy_decode(model, src, src_mask, max_len=64, start_symbol=2, end_symbol=3):\n",
    "    memory = model.encode(src, src_mask)\n",
    "    # Initialize the prediction content as a 1×1 tensor, fill it with the ID of the start symbol ('BOS'), and set the type to the input data type (LongTensor).\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    # Iterate over the length subscript of the output\n",
    "    for i in range(max_len - 1):\n",
    "        # decode obtains the hidden layer representation\n",
    "        out = model.decode(memory,\n",
    "                           src_mask,\n",
    "                           Variable(ys),\n",
    "                           Variable(subsequent_mask(ys.size(1)).type_as(src.data)))\n",
    "        # convert the hidden representation into a log-softmax probability distribution over the words in the dictionary.\n",
    "        prob = model.generator(out[:, -1])\n",
    "        # obtain the predicted word ID with the highest probability at the current position.\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.data[0]\n",
    "        if next_word == end_symbol:\n",
    "            break\n",
    "        # concatenate the predicted character ID at the current position with the previously predicted content.\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47e2a8c",
   "metadata": {},
   "source": [
    "## Beam search\n",
    "Beam search is a search algorithm commonly used in natural language processing and machine translation to find the most likely sequence of words given a set of possible choices. It works by exploring a set of candidate solutions and gradually narrowing down the options by selecting only the most promising ones based on a certain scoring function. This approach is particularly useful in cases where the search space is large and exhaustive search is not feasible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d8f355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Beam:\n",
    "    \"\"\" Beam search \"\"\"\n",
    "\n",
    "    def __init__(self, size, pad, bos, eos, device=False):\n",
    "\n",
    "        self.size = size\n",
    "        self._done = False\n",
    "        self.PAD = pad\n",
    "        self.BOS = bos\n",
    "        self.EOS = eos\n",
    "        # The score for each translation on the beam.\n",
    "        self.scores = torch.zeros((size,), dtype=torch.float, device=device)\n",
    "        self.all_scores = []\n",
    "\n",
    "        # The backpointers at each time-step.\n",
    "        self.prev_ks = []\n",
    "\n",
    "        # The outputs at each time-step.\n",
    "        # Initialize to [BOS, PAD, PAD ..., PAD]\n",
    "        self.next_ys = [torch.full((size,), self.PAD, dtype=torch.long, device=device)]\n",
    "        self.next_ys[0][0] = self.BOS\n",
    "\n",
    "    def get_current_state(self):\n",
    "        \"\"\"Get the outputs for the current timestep.\"\"\"\n",
    "        return self.get_tentative_hypothesis()\n",
    "\n",
    "    def get_current_origin(self):\n",
    "        \"\"\"Get the backpointers for the current timestep.\"\"\"\n",
    "        return self.prev_ks[-1]\n",
    "\n",
    "    @property\n",
    "    def done(self):\n",
    "        return self._done\n",
    "\n",
    "    def advance(self, word_logprob):\n",
    "        \"\"\"Update beam status and check if finished or not.\"\"\"\n",
    "        num_words = word_logprob.size(1)\n",
    "\n",
    "        # Sum the previous scores.\n",
    "        if len(self.prev_ks) > 0:\n",
    "            beam_lk = word_logprob + self.scores.unsqueeze(1).expand_as(word_logprob)\n",
    "        else:\n",
    "            # in initial case,\n",
    "            beam_lk = word_logprob[0]\n",
    "\n",
    "        flat_beam_lk = beam_lk.view(-1)\n",
    "        best_scores, best_scores_id = flat_beam_lk.topk(self.size, 0, True, True)\n",
    "\n",
    "        self.all_scores.append(self.scores)\n",
    "        self.scores = best_scores\n",
    "\n",
    "        # bestScoresId is flattened as a (beam x word) array,\n",
    "        # so we need to calculate which word and beam each score came from\n",
    "        prev_k = best_scores_id // num_words\n",
    "        self.prev_ks.append(prev_k)\n",
    "        self.next_ys.append(best_scores_id - prev_k * num_words)\n",
    "\n",
    "        # End condition is when top-of-beam is EOS.\n",
    "        if self.next_ys[-1][0].item() == self.EOS:\n",
    "            self._done = True\n",
    "            self.all_scores.append(self.scores)\n",
    "\n",
    "        return self._done\n",
    "\n",
    "    def sort_scores(self):\n",
    "        \"\"\"Sort the scores.\"\"\"\n",
    "        return torch.sort(self.scores, 0, True)\n",
    "\n",
    "    def get_the_best_score_and_idx(self):\n",
    "        \"\"\"Get the score of the best in the beam.\"\"\"\n",
    "        scores, ids = self.sort_scores()\n",
    "        return scores[1], ids[1]\n",
    "\n",
    "    def get_tentative_hypothesis(self):\n",
    "        \"\"\"Get the decoded sequence for the current timestep.\"\"\"\n",
    "\n",
    "        if len(self.next_ys) == 1:\n",
    "            dec_seq = self.next_ys[0].unsqueeze(1)\n",
    "        else:\n",
    "            _, keys = self.sort_scores()\n",
    "            hyps = [self.get_hypothesis(k) for k in keys]\n",
    "            hyps = [[self.BOS] + h for h in hyps]\n",
    "            dec_seq = torch.LongTensor(hyps)\n",
    "\n",
    "        return dec_seq\n",
    "\n",
    "    def get_hypothesis(self, k):\n",
    "        \"\"\" Walk back to construct the full hypothesis. \"\"\"\n",
    "        # print(k.type())\n",
    "        hyp = []\n",
    "        for j in range(len(self.prev_ks) - 1, -1, -1):\n",
    "            hyp.append(self.next_ys[j + 1][k])\n",
    "            k = self.prev_ks[j][k]\n",
    "\n",
    "        return list(map(lambda x: x.item(), hyp[::-1]))\n",
    "\n",
    "\n",
    "def beam_search(model, src, src_mask, max_len, pad, bos, eos, beam_size, device):\n",
    "    \"\"\" Translation work in one batch \"\"\"\n",
    "\n",
    "    def get_inst_idx_to_tensor_position_map(inst_idx_list):\n",
    "        \"\"\" Indicate the position of an instance in a tensor. \"\"\"\n",
    "        return {inst_idx: tensor_position for tensor_position, inst_idx in enumerate(inst_idx_list)}\n",
    "\n",
    "    def collect_active_part(beamed_tensor, curr_active_inst_idx, n_prev_active_inst, n_bm):\n",
    "        \"\"\" Collect tensor parts associated to active instances. \"\"\"\n",
    "\n",
    "        _, *d_hs = beamed_tensor.size()\n",
    "        n_curr_active_inst = len(curr_active_inst_idx)\n",
    "        # active instances (elements of batch) * beam search size x seq_len x h_dimension\n",
    "        new_shape = (n_curr_active_inst * n_bm, *d_hs)\n",
    "\n",
    "        # select only parts of tensor which are still active\n",
    "        beamed_tensor = beamed_tensor.view(n_prev_active_inst, -1)\n",
    "        beamed_tensor = beamed_tensor.index_select(0, curr_active_inst_idx)\n",
    "        beamed_tensor = beamed_tensor.view(*new_shape)\n",
    "\n",
    "        return beamed_tensor\n",
    "\n",
    "    def collate_active_info(\n",
    "            src_enc, src_mask, inst_idx_to_position_map, active_inst_idx_list):\n",
    "        # Sentences which are still active are collected,\n",
    "        # so the decoder will not run on completed sentences.\n",
    "        n_prev_active_inst = len(inst_idx_to_position_map)\n",
    "        active_inst_idx = [inst_idx_to_position_map[k] for k in active_inst_idx_list]\n",
    "        active_inst_idx = torch.LongTensor(active_inst_idx).to(device)\n",
    "\n",
    "        active_src_enc = collect_active_part(src_enc, active_inst_idx, n_prev_active_inst, beam_size)\n",
    "        active_inst_idx_to_position_map = get_inst_idx_to_tensor_position_map(active_inst_idx_list)\n",
    "        active_src_mask = collect_active_part(src_mask, active_inst_idx, n_prev_active_inst, beam_size)\n",
    "\n",
    "        return active_src_enc, active_src_mask, active_inst_idx_to_position_map\n",
    "\n",
    "    def beam_decode_step(\n",
    "            inst_dec_beams, len_dec_seq, enc_output, inst_idx_to_position_map, n_bm):\n",
    "        \"\"\" Decode and update beam status, and then return active beam idx \"\"\"\n",
    "\n",
    "        def prepare_beam_dec_seq(inst_dec_beams, len_dec_seq):\n",
    "            dec_partial_seq = [b.get_current_state() for b in inst_dec_beams if not b.done]\n",
    "            # Batch size x Beam size x Dec Seq Len\n",
    "            dec_partial_seq = torch.stack(dec_partial_seq).to(device)\n",
    "            # Batch size*Beam size x Dec Seq Len\n",
    "            dec_partial_seq = dec_partial_seq.view(-1, len_dec_seq)\n",
    "            return dec_partial_seq\n",
    "\n",
    "        def predict_word(dec_seq, enc_output, n_active_inst, n_bm):\n",
    "            assert enc_output.shape[0] == dec_seq.shape[0] == src_mask.shape[0]\n",
    "            out = model.decode(enc_output, src_mask,\n",
    "                               dec_seq,\n",
    "                               subsequent_mask(dec_seq.size(1))\n",
    "                               .type_as(src.data))\n",
    "            word_logprob = model.generator(out[:, -1])\n",
    "            word_logprob = word_logprob.view(n_active_inst, n_bm, -1)\n",
    "\n",
    "            return word_logprob\n",
    "\n",
    "        def collect_active_inst_idx_list(inst_beams, word_prob, inst_idx_to_position_map):\n",
    "            active_inst_idx_list = []\n",
    "            for inst_idx, inst_position in inst_idx_to_position_map.items():\n",
    "                is_inst_complete = inst_beams[inst_idx].advance(\n",
    "                    word_prob[inst_position])  # Fill Beam object with assigned probabilities\n",
    "                if not is_inst_complete:  # if top beam ended with eos, we do not add it\n",
    "                    active_inst_idx_list += [inst_idx]\n",
    "\n",
    "            return active_inst_idx_list\n",
    "\n",
    "        n_active_inst = len(inst_idx_to_position_map)\n",
    "\n",
    "        # get decoding sequence for each beam\n",
    "        # size: Batch size*Beam size x Dec Seq Len\n",
    "        dec_seq = prepare_beam_dec_seq(inst_dec_beams, len_dec_seq)\n",
    "\n",
    "        # get word probabilities for each beam\n",
    "        # size: Batch size x Beam size x Vocabulary\n",
    "        word_logprob = predict_word(dec_seq, enc_output, n_active_inst, n_bm)\n",
    "\n",
    "        # Update the beam with predicted word prob information and collect incomplete instances\n",
    "        active_inst_idx_list = collect_active_inst_idx_list(\n",
    "            inst_dec_beams, word_logprob, inst_idx_to_position_map)\n",
    "\n",
    "        return active_inst_idx_list\n",
    "\n",
    "    def collect_hypothesis_and_scores(inst_dec_beams, n_best):\n",
    "        all_hyp, all_scores = [], []\n",
    "        for inst_idx in range(len(inst_dec_beams)):\n",
    "            scores, tail_idxs = inst_dec_beams[inst_idx].sort_scores()\n",
    "            all_scores += [scores[:n_best]]\n",
    "\n",
    "            hyps = [inst_dec_beams[inst_idx].get_hypothesis(i) for i in tail_idxs[:n_best]]\n",
    "            all_hyp += [hyps]\n",
    "        return all_hyp, all_scores\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # -- Encode\n",
    "        src_enc = model.encode(src, src_mask)\n",
    "\n",
    "        #  Repeat data for beam search\n",
    "        NBEST = beam_size\n",
    "        batch_size, sent_len, h_dim = src_enc.size()\n",
    "        src_enc = src_enc.repeat(1, beam_size, 1).view(batch_size * beam_size, sent_len, h_dim)\n",
    "        src_mask = src_mask.repeat(1, beam_size, 1).view(batch_size * beam_size, 1, src_mask.shape[-1])\n",
    "\n",
    "        # -- Prepare beams\n",
    "        inst_dec_beams = [Beam(beam_size, pad, bos, eos, device) for _ in range(batch_size)]\n",
    "\n",
    "        # -- Bookkeeping for active or not\n",
    "        active_inst_idx_list = list(range(batch_size))\n",
    "        inst_idx_to_position_map = get_inst_idx_to_tensor_position_map(active_inst_idx_list)\n",
    "\n",
    "        # -- Decode\n",
    "        for len_dec_seq in range(1, max_len + 1):\n",
    "\n",
    "            active_inst_idx_list = beam_decode_step(\n",
    "                inst_dec_beams, len_dec_seq, src_enc, inst_idx_to_position_map, beam_size)\n",
    "\n",
    "            if not active_inst_idx_list:\n",
    "                break  # all instances have finished their path to <EOS>\n",
    "            # filter out inactive tensor parts (for already decoded sequences)\n",
    "            src_enc, src_mask, inst_idx_to_position_map = collate_active_info(\n",
    "                src_enc, src_mask, inst_idx_to_position_map, active_inst_idx_list)\n",
    "\n",
    "    batch_hyp, batch_scores = collect_hypothesis_and_scores(inst_dec_beams, NBEST)\n",
    "\n",
    "    return batch_hyp, batch_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2b8d62",
   "metadata": {},
   "source": [
    "## BLEU score\n",
    "BLEU (Bilingual Evaluation Understudy) score is a metric commonly used to evaluate the quality of machine-translated text by comparing it to one or more reference translations. It measures the overlap between the machine-generated text and the reference translations, taking into account the precision of n-grams (sequences of n words) and the brevity penalty. The higher the BLEU score, the better the translation\n",
    "quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd866b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import logging\n",
    "import sacrebleu\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "def run_epoch(data, model, loss_compute):\n",
    "    total_tokens = 0.\n",
    "    total_loss = 0.\n",
    "\n",
    "    for batch in tqdm(data):\n",
    "        out = model(batch.src, batch.trg, batch.src_mask, batch.trg_mask)\n",
    "        loss = loss_compute(out, batch.trg_y, batch.ntokens)\n",
    "\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "    return total_loss / total_tokens\n",
    "\n",
    "\n",
    "def train(train_data, dev_data, model, model_par, criterion, optimizer):\n",
    "    \"\"\"train and save model\"\"\"\n",
    "    best_bleu_score = 0.0\n",
    "    bleu_score_list = []\n",
    "    dev_loss_list = []\n",
    "    early_stop = config.early_stop\n",
    "    for epoch in range(1, config.epoch_num + 1):\n",
    "        # train the model\n",
    "        model.train()\n",
    "        train_loss = run_epoch(train_data, model_par,\n",
    "                               MultiGPULossCompute(model.generator, criterion, config.device_id, optimizer))\n",
    "        logging.info(\"Epoch: {}, loss: {}\".format(epoch, train_loss))\n",
    "        # model validation\n",
    "        model.eval()\n",
    "        dev_loss = run_epoch(dev_data, model_par,\n",
    "                             MultiGPULossCompute(model.generator, criterion, config.device_id, None))\n",
    "        bleu_score = evaluate(dev_data, model)\n",
    "        \n",
    "        dev_loss_list.append(dev_loss.cpu().detach().numpy())\n",
    "        bleu_score_list.append(bleu_score)\n",
    "        \n",
    "        logging.info('Epoch: {}, Dev loss: {}, Bleu Score: {}'.format(epoch, dev_loss, bleu_score))\n",
    "\n",
    "        # save the current model if its loss on the dev set for the current epoch is better than the previously recorded best loss, and update the best loss value.\n",
    "        if bleu_score > best_bleu_score:\n",
    "            torch.save(model.state_dict(), config.output_model_path)\n",
    "            best_bleu_score = bleu_score\n",
    "            early_stop = config.early_stop\n",
    "            logging.info(\"-------- Save Best Model! --------\")\n",
    "        else:\n",
    "            early_stop -= 1\n",
    "            logging.info(\"Early Stop Left: {}\".format(early_stop))\n",
    "        if early_stop == 0:\n",
    "            logging.info(\"-------- Early Stop! --------\")\n",
    "            break\n",
    "    return dev_loss_list, bleu_score_list\n",
    "\n",
    "\n",
    "class LossCompute:\n",
    "    \"\"\"A single-gpu loss compute and train function.\"\"\"\n",
    "\n",
    "    def __init__(self, generator, criterion, opt=None):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "\n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)),\n",
    "                              y.contiguous().view(-1)) / norm\n",
    "        loss.backward()\n",
    "        if self.opt is not None:\n",
    "            self.opt.step()\n",
    "            if config.use_noamopt:\n",
    "                self.opt.optimizer.zero_grad()\n",
    "            else:\n",
    "                self.opt.zero_grad()\n",
    "        return loss.data.item() * norm.float()\n",
    "\n",
    "class MultiGPULossCompute:\n",
    "    \"\"\"A multi-gpu loss compute and train function.\"\"\"\n",
    "\n",
    "    def __init__(self, generator, criterion, devices, opt=None, chunk_size=5):\n",
    "        # Send out to different gpus.\n",
    "        self.generator = generator\n",
    "        self.criterion = nn.parallel.replicate(criterion, devices=devices)\n",
    "        self.opt = opt\n",
    "        self.devices = devices\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "    def __call__(self, out, targets, normalize):\n",
    "        total = 0.0\n",
    "        generator = nn.parallel.replicate(self.generator, devices=self.devices)\n",
    "        out_scatter = nn.parallel.scatter(out, target_gpus=self.devices)\n",
    "        out_grad = [[] for _ in out_scatter]\n",
    "        targets = nn.parallel.scatter(targets, target_gpus=self.devices)\n",
    "\n",
    "        # Divide generating into chunks.\n",
    "        chunk_size = self.chunk_size\n",
    "        for i in range(0, out_scatter[0].size(1), chunk_size):\n",
    "            # Predict distributions\n",
    "            out_column = [[Variable(o[:, i:i + chunk_size].data,\n",
    "                                    requires_grad=self.opt is not None)]\n",
    "                          for o in out_scatter]\n",
    "            gen = nn.parallel.parallel_apply(generator, out_column)\n",
    "\n",
    "            # Compute loss.\n",
    "            y = [(g.contiguous().view(-1, g.size(-1)),\n",
    "                  t[:, i:i + chunk_size].contiguous().view(-1))\n",
    "                 for g, t in zip(gen, targets)]\n",
    "            loss = nn.parallel.parallel_apply(self.criterion, y)\n",
    "\n",
    "            # Sum and normalize loss\n",
    "            l_ = nn.parallel.gather(loss, target_device=self.devices[0])\n",
    "            l_ = l_.sum() / normalize\n",
    "            total += l_.data\n",
    "\n",
    "            # Backprop loss to output of transformer\n",
    "            if self.opt is not None:\n",
    "                l_.backward()\n",
    "                for j, l in enumerate(loss):\n",
    "                    out_grad[j].append(out_column[j][0].grad.data.clone())\n",
    "\n",
    "        # Backprop all loss through transformer.\n",
    "        if self.opt is not None:\n",
    "            out_grad = [Variable(torch.cat(og, dim=1)) for og in out_grad]\n",
    "            o1 = out\n",
    "            o2 = nn.parallel.gather(out_grad,\n",
    "                                    target_device=self.devices[0])\n",
    "            o1.backward(gradient=o2)\n",
    "            self.opt.step()\n",
    "            if config.use_noamopt:\n",
    "                self.opt.optimizer.zero_grad()\n",
    "            else:\n",
    "                self.opt.zero_grad()\n",
    "        return total * normalize\n",
    "\n",
    "    \n",
    "def evaluate(data, model, mode='eval', use_beam=True):\n",
    "    \"\"\"Predict using the trained model and print the output\"\"\"\n",
    "    sp_chn = chinese_tokenizer_load()\n",
    "    engs = []\n",
    "    trg = []\n",
    "    res = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data):\n",
    "            en_sent = batch.src_text\n",
    "            cn_sent = batch.trg_text\n",
    "            src = batch.src\n",
    "            src_mask = (src != 0).unsqueeze(-2)\n",
    "            if use_beam:\n",
    "                decode_result, _ = beam_search(model, src, src_mask, config.max_len,\n",
    "                                               config.padding_idx, config.bos_idx, config.eos_idx,\n",
    "                                               config.beam_size, device)\n",
    "            else:\n",
    "                decode_result = batch_greedy_decode(model, src, src_mask,\n",
    "                                                    max_len=config.max_len)\n",
    "            decode_result = [h[0] for h in decode_result]\n",
    "            translation = [sp_chn.decode_ids(_s) for _s in decode_result]\n",
    "            trg.extend(cn_sent)\n",
    "            res.extend(translation)\n",
    "            engs.extend(en_sent)\n",
    "    if mode == 'test':\n",
    "        for i in range(len(trg)):\n",
    "            line = \"idx: \\n \" + str(i) +': \\n' + engs[i] +'\\n label: '+trg[i] + '\\n predict:' + res[i] + '\\n'\n",
    "            print(line)     \n",
    "    trg = [trg]\n",
    "    bleu = sacrebleu.corpus_bleu(res, trg, tokenize='zh')\n",
    "    return float(bleu.score)\n",
    "\n",
    "\n",
    "def test(data, model, criterion, mode='eval'):\n",
    "    with torch.no_grad():\n",
    "        # load model\n",
    "        model.load_state_dict(torch.load(config.output_model_path))\n",
    "        model_par = torch.nn.DataParallel(model)\n",
    "        model.eval()\n",
    "        # predict\n",
    "        test_loss = run_epoch(data, model_par,\n",
    "                              MultiGPULossCompute(model.generator, criterion, config.device_id, None))\n",
    "        bleu_score = evaluate(data, model, mode)\n",
    "        logging.info('Test loss: {},  Bleu Score: {}'.format(test_loss, bleu_score))\n",
    "    return test_loss.cpu().detach().numpy(), bleu_score\n",
    "\n",
    "\n",
    "def translate(src, model, model_path,  use_beam=True):\n",
    "    \"\"\"Predict a single sentence using the trained model and print the output.\"\"\"\n",
    "    sp_chn = chinese_tokenizer_load()\n",
    "    with torch.no_grad():\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        model.eval()\n",
    "        src_mask = (src != 0).unsqueeze(-2)\n",
    "        if use_beam:\n",
    "            decode_result, _ = beam_search(model, src, src_mask, config.max_len,\n",
    "                                           config.padding_idx, config.bos_idx, config.eos_idx,\n",
    "                                           config.beam_size, device)\n",
    "            decode_result = [h[0] for h in decode_result]\n",
    "        else:\n",
    "            decode_result = batch_greedy_decode(model, src, src_mask, max_len=config.max_len)\n",
    "        translation = [sp_chn.decode_ids(_s) for _s in decode_result]\n",
    "        return translation[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4e2090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "class NoamOpt:\n",
    "    \"\"\"Optim wrapper that implements rate.\"\"\"\n",
    "\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Update parameters and rate\"\"\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def rate(self, step=None):\n",
    "        \"\"\"Implement `lrate` above\"\"\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * (self.model_size ** (-0.5) * min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
    "\n",
    "\n",
    "def get_std_opt(model):\n",
    "    \"\"\"for batch_size 32, 5530 steps for one epoch, 2 epoch for warm-up\"\"\"\n",
    "    return NoamOpt(model.src_embed[0].d_model, 1, 10000,\n",
    "                   torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "\n",
    "def run():\n",
    "    set_logger(config.log_path)\n",
    "\n",
    "    train_dataset = MTDataset(config.train_data_path)\n",
    "    dev_dataset = MTDataset(config.dev_data_path)\n",
    "    test_dataset = MTDataset(config.test_data_path)\n",
    "\n",
    "    logging.info(\"-------- Dataset Build! --------\")\n",
    "    train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=config.batch_size,\n",
    "                                  collate_fn=train_dataset.collate_fn)\n",
    "    dev_dataloader = DataLoader(dev_dataset, shuffle=False, batch_size=config.batch_size,\n",
    "                                collate_fn=dev_dataset.collate_fn)\n",
    "    test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=config.batch_size,\n",
    "                                 collate_fn=test_dataset.collate_fn)\n",
    "\n",
    "    logging.info(\"-------- Get Dataloader! --------\")\n",
    "    # initialize the model\n",
    "    model = make_model(config.src_vocab_size, config.tgt_vocab_size, config.n_layers,\n",
    "                       config.d_model, config.d_ff, config.n_heads, config.dropout)\n",
    "    model_par = torch.nn.DataParallel(model)\n",
    "    # train the model\n",
    "    if config.use_smoothing:\n",
    "        criterion = LabelSmoothing(size=config.tgt_vocab_size, padding_idx=config.padding_idx, smoothing=0.1)\n",
    "        criterion.cuda()\n",
    "    else:\n",
    "        criterion = torch.nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n",
    "    if config.use_noamopt:\n",
    "        optimizer = get_std_opt(model)\n",
    "    else:\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr)\n",
    "    train_loss_list, train_bleu_score_list= train(train_dataloader, dev_dataloader, model, model_par, criterion, optimizer)\n",
    "    test_loss, test_bleu_score = test(test_dataloader, model, criterion)\n",
    "    \n",
    "    return train_loss_list, train_bleu_score_list, test_loss, test_bleu_score\n",
    "\n",
    "\n",
    "def check_opt():\n",
    "    \"\"\"check learning rate changes\"\"\"\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    model = make_model(config.src_vocab_size, config.tgt_vocab_size, config.n_layers,\n",
    "                       config.d_model, config.d_ff, config.n_heads, config.dropout)\n",
    "    opt = get_std_opt(model)\n",
    "    # Three settings of the lrate hyperparameters.\n",
    "    opts = [opt,\n",
    "            NoamOpt(512, 1, 20000, None),\n",
    "            NoamOpt(256, 1, 10000, None)]\n",
    "    plt.plot(np.arange(1, 50000), [[opt.rate(i) for opt in opts] for i in range(1, 50000)])\n",
    "    plt.legend([\"512:10000\", \"512:20000\", \"256:10000\"])\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def one_sentence_translate(sent, model_path,beam_search=True):\n",
    "    # model initialation\n",
    "    model = make_model(config.src_vocab_size, config.tgt_vocab_size, config.n_layers,\n",
    "                       config.d_model, config.d_ff, config.n_heads, config.dropout)\n",
    "    BOS = english_tokenizer_load().bos_id()  # 2\n",
    "    EOS = english_tokenizer_load().eos_id()  # 3\n",
    "    src_tokens = [[BOS] + english_tokenizer_load().EncodeAsIds(sent) + [EOS]]\n",
    "    batch_input = torch.LongTensor(np.array(src_tokens)).to(device)\n",
    "    return translate(batch_input, model, model_path, use_beam=beam_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372ee71f",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9723c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list, train_bleu_score_list, test_loss, test_bleu_score = run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d637a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('Test loss: {},  Bleu Score: {}'.format(test_loss, test_bleu_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e5412c",
   "metadata": {},
   "source": [
    "## Visualize the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29638451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs_list = list(range(len(train_loss_list)))\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.plot(epochs_list, train_loss_list)\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0114c66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_list = list(range(len(train_bleu_score_list)))\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.plot(epochs_list, train_bleu_score_list)\n",
    "plt.title('Model Bleu Score')\n",
    "plt.ylabel('Bleu Score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daebb3e",
   "metadata": {},
   "source": [
    "## Test trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33f13ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_example(model_path=config.output_model_path):\n",
    "    \"\"\"单句翻译示例\"\"\" \n",
    "    sent = \"I love Xiamen University\"\n",
    "    tgt= \"我爱厦门大学\"\n",
    "    res = one_sentence_translate(sent, model_path,beam_search=False)\n",
    "    print(f'{sent} => {res} \\n label: {tgt} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b7f5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28da304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataset():\n",
    "    test_dataset = MTDataset(config.test_data_path)\n",
    "\n",
    "    logging.info(\"-------- Dataset Build! --------\")\n",
    "    test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=config.batch_size,\n",
    "                                 collate_fn=test_dataset.collate_fn)\n",
    "\n",
    "    logging.info(\"-------- Get Dataloader! --------\")\n",
    "    # initialize the model\n",
    "    model = make_model(config.src_vocab_size, config.tgt_vocab_size, config.n_layers,\n",
    "                       config.d_model, config.d_ff, config.n_heads, config.dropout)\n",
    "    model_par = torch.nn.DataParallel(model)\n",
    "    # train the model\n",
    "    if config.use_smoothing:\n",
    "        criterion = LabelSmoothing(size=config.tgt_vocab_size, padding_idx=config.padding_idx, smoothing=0.1)\n",
    "        criterion.cuda()\n",
    "    else:\n",
    "        criterion = torch.nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n",
    "    test_loss, test_bleu_score = test(test_dataloader, model, criterion, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65b0bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bd12e7",
   "metadata": {},
   "source": [
    "## Question(40 marks)\n",
    "1. Why can transformer train in parallel but not reference in parallel? (5 marks)\n",
    "\n",
    "Answer:\n",
    "\n",
    "2. What is the relationship between the convolution operations and the attention operations? (10 marks)\n",
    "\n",
    "Answer:\n",
    "\n",
    "3. Why is a mask needed after tokenization? Attention mechanisms also use masks, what are their functions respectively? (10 marks)\n",
    "\n",
    "Answer:\n",
    "\n",
    "4. Why does Transformer introduce positional coding? Why  do RNN, GRU, LSTM not need to introduce positional coding? (5 marks)\n",
    "\n",
    "Answer:\n",
    "\n",
    "5. After you finish your assignment, please describe the whole process of machine translation based transformer, in other word, how is an English sentence  translated into Chinese ? The more detailed, the better. (10 marks)\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d72c723",
   "metadata": {},
   "source": [
    "## Last but not least\n",
    "When you finish this assignment, you got the understanding that the Transformer model consists of the Encoder module and the Decoder module, however the encoder-decoder models are one of the models in large languages models(LLM); the Encoder module and Decoder module can be used individually.\n",
    "We would like to suggest you to read the following papers\n",
    "- Encoder-only: [BERT](https://aclanthology.org/N19-1423.pdf), [ViT](https://arxiv.org/pdf/2010.11929.pdf)\n",
    "- Decoder-pnly: [GPT-1/2/3/4](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf), [ChatGPT](https://openai.com/research/gpt-4)\n",
    "- Encoder-Decoder: [T5](https://jmlr.org/papers/v21/20-074.html)\n",
    "\n",
    "For more LLM, please refer to [A Survey of Large Language Models](https://arxiv.org/pdf/2303.18223.pdf) and [Awesome-LLM](https://github.com/Hannibal046/Awesome-LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475101bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
